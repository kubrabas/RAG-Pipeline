{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPzlHy8JRoFX"
   },
   "source": [
    "Using the last cell of this notebook, you can ask questions about the PDFs to our LLM model, which is improved step by step throughout the notebook.  Optionally, you can also inquire about the page from which our LLM model retrieved the information to provide the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "veNysdJiWnJX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import perf_counter as timer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vX2Dxs7aQ0r2",
    "outputId": "86a03f29-cb99-43ec-b29f-f92926e613a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 22 GB\n"
     ]
    }
   ],
   "source": [
    "# Get GPU available memory\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "zzX6pRn4R6wd",
    "outputId": "4cf6f45d-29cc-4e87-9fa0-72b516b095dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-0ed2305f-ddf5-48a6-bc61-ef84fb3fb9d0\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-0ed2305f-ddf5-48a6-bc61-ef84fb3fb9d0\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunks_and_embeddings_for_WeitBlick.csv to chunks_and_embeddings_for_WeitBlick.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS6UJbUcRIwK"
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWQaxKNTQ8ro",
    "outputId": "5446b4da-7483-4dc6-f020-e053d0e471ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([120, 768])\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "# Import texts and embedding df\n",
    "chunks_and_embeddings_df = pd.read_csv(\"chunks_and_embeddings.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "chunks_and_embeddings_df[\"embedding\"] = chunks_and_embeddings_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "chunks_and_embeddings = chunks_and_embeddings_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(chunks_and_embeddings_df[\"embedding\"].tolist()), dtype=torch.float16).to(device)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8u6CmgZVEdt",
    "outputId": "6147b3a7-fe9a-4c7e-94a1-32eccf8dd5c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.40)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qCtRE9ZSFrk",
    "outputId": "290d11de-2293-48d6-d897-b06b166852a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbsfmt8aBUYR"
   },
   "source": [
    "Embedding model is ready. Let's perform a semantic search. We can do so with the following steps:\n",
    "\n",
    "- Define a query string. Turn the query string in an embedding with same model we used to embed our text chunks.\n",
    "\n",
    "- Perform a dot product or cosine similarity function between the text embeddings and the query embedding to get similarity scores.\n",
    "\n",
    "- Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svgodunsSFt3",
    "outputId": "f16f432a-0661-4e27-cd96-4796d2f678c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: was müssen sie bei teilzahlungen beachten?\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the query\n",
    "query = \"was müssen sie bei teilzahlungen beachten?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples\n",
    "# Note: It's important to embed the query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wyx9VnzVSFws",
    "outputId": "26d5d088-69ad-45b3-dc67-80eb408c99c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to get scores on 120 embeddings: 0.00083 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.5757, 0.5132, 0.4565, 0.4204, 0.3979], device='cuda:0',\n",
       "       dtype=torch.float16),\n",
       "indices=tensor([21, 59, 10, 35, 60], device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Get similarity scores with the dot product.\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "## can use dot score rather than cosine similarity, because our embeddings are normalized already\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdDCboHhCsF2"
   },
   "source": [
    "Let's check the results of our\n",
    "similarity search. torch.topk returns a tuple of values (scores) and indicies for those scores. The indicies relate to which indicies in the embeddings tensor have what scores in relation to the query embedding (higher is better). We can use those indicies to map back to our text chunks. Let's first define helper function to print wrapped text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8-frHvbVSFzt"
   },
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5uyBRpsSF2S",
    "outputId": "16bd4c23-7d1c-41d5-c507-d529d71753c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'was müssen sie bei teilzahlungen beachten?'\n",
      "\n",
      "Results:\n",
      "Score: 0.5757\n",
      "Text:\n",
      "10 6.3 Wie und wann leisten wir die Auszahlung? 10 6.4 Wer erhält die\n",
      "Auszahlung?10 7 Zuzahlung; Teilauszahlung und Auszahlungsplan; Verlegung des\n",
      "Ablaufdatums; Kündigung 10 7.1 Was müssen Sie beachten, wenn Sie Zuzahlungen\n",
      "leisten möchten? 10 7.2 Was müssen Sie bei Teilauszahlungen beachten?11 7.3 Was\n",
      "ist der Auszahlungsplan? 11 7.4 Können Sie das Ablaufdatum verschieben und was\n",
      "hat das für Folgen? 12 7.5 Was müssen Sie beachten, wenn Sie Ihren Vertrag\n",
      "kündigen? 12 8 Kosten 13 8.1 Welche Kosten fallen für Ihren Vertrag an?\n",
      "Page number: 11\n",
      "\n",
      "\n",
      "Score: 0.5132\n",
      "Text:\n",
      "Wenn Sie sich dazu nicht äußern, verteilen wir Ihre Zuzahlung entsprechend dem\n",
      "Verhältnis, welches Sie bei Vertragsabschluss für Ihren Einmalbeitrag gewählt\n",
      "haben.g) Für jede Zuzahlung kann Startmanagement gewählt werden (5.9).h) Der →\n",
      "maßgebliche Stichtag für die Berechnung der → Anteilseinheiten, die sich aus\n",
      "Ihrer Zuzahlung ergeben, ist der gewünschte Termin oder der zweite → Handelstag,\n",
      "nachdem wir Ihre Zuzahlung erhalten haben, je nachdem, welcher Termin später\n",
      "ist.Ist der Stichtag kein → Handelstag, ist der nächstmögliche → Handelstag\n",
      "maßgeblich.7.2 Was müssen Sie bei Teilauszahlungen beachten? a) Teilauszahlungen\n",
      "sind unter folgenden Voraussetzungen möglich:  Sie müssen sie in → Textform bei\n",
      "uns beantragen. Eine Teilauszahlung ist frühestens einen Monat nach\n",
      "Vertragsabschluss und spätestens einen Monat vor Vertragsablauf möglich. Es ist\n",
      "nur eine Teilauszahlung monatlich möglich, jedoch nicht mehr als zwei im Jahr.\n",
      "Page number: 23\n",
      "\n",
      "\n",
      "Score: 0.4565\n",
      "Text:\n",
      "Steuerinformationen Seite 2 von 3 /D/1006/XIII/03/22 1.3 Was müssen Sie bei\n",
      "Vertragsänderungen beachten?Bitte beachten Sie auch, dass Änderungen an Ihrem\n",
      "Vertrag dazu führen können, dass Zahlungen aus einer Kapitallebensversicherung\n",
      "im Erlebensfall, bei Teilauszahlung sowie bei Rückkauf des Vertrags ganz oder\n",
      "teilweise mit dem vollen Unterschiedsbetrag der Steuerpflicht unterliegen. 1.4\n",
      "Veräußerung einer Versicherungspolice Bei Veräußerung einer Lebensversicherung\n",
      "müssen wir als Versicherungsunternehmen nach § 20 Abs. 2 Nr. 6 EStG eine Meldung\n",
      "an das Finanzamt, das für den Steuerpflichtigen (= Veräußerer) zuständig ist,\n",
      "vornehmen.Auf Verlangen des Steuerpflichtigen stellen wir eine Bescheinigung\n",
      "über die Höhe der entrichteten Beiträge zum Zeitpunkt der Veräußerung aus.1.5\n",
      "Steuerpflichtiger Steuerpflichtiger im Sinne des § 20 Abs. 1 Nr. 6 EStG ist\n",
      "grundsätzlich derjenige, der das Kapital in Form der Sparanteile im eigenen\n",
      "Namen und für eigene Rechnung dem Versicherungsunternehmen zur Nutzung\n",
      "überlassen hat. In der Regel sind Versicherungsnehmer Steuerpflichtige – bei\n",
      "zwei Versicherungsnehmern entsprechend ihren Anteilen am Vertrag –, da sie die\n",
      "Sparanteile zur Nutzung überlassen haben und auch Inhaber des Rechts sind, die\n",
      "Versicherungsleistung zu fordern. Mit der Einräumung eines unwiderruflichen\n",
      "Bezugsrechts für die steuerpflichtige Versicherungsleistung gilt grundsätzlich\n",
      "der Bezugsberechtigte als Steuerpflichtiger der erzielten Erträge. 1.6\n",
      "Erbschaftsteuer und Schenkungsteuer Versicherungsleistungen, die an\n",
      "Versicherungsnehmer selbst gezahlt werden, sind erbschaftsteuerfrei.\n",
      "Page number: 8\n",
      "\n",
      "\n",
      "Score: 0.4204\n",
      "Text:\n",
      "Wir bleiben aber auch in diesem Fall zur Leistung verpflichtet, wenn uns\n",
      "nachgewiesen wird, dass Sie die Nichtzahlung nicht zu vertreten haben.\n",
      "Page number: 15\n",
      "\n",
      "\n",
      "Score: 0.3979\n",
      "Text:\n",
      " Nach jeder Teilauszahlung müssen noch 5.000 Euro im Vertrag verbleiben. Eine\n",
      "Teilauszahlung muss mindestens 1.500 Euro betragen, maximal jedoch 1.000.000\n",
      "Euro.b) Durch eine Teilauszahlung reduzieren sich die Versicherungsleistungen\n",
      "und die Anteile an den Fonds, die Sie gewählt haben.c) Sie können entscheiden,\n",
      "aus welchen Fonds Sie die Teilauszahlung entnehmen möchten.Tun Sie das nicht,\n",
      "entnehmen wir die Beträge entsprechend dem Verhältnis, in dem Ihr Fondsvermögen\n",
      "am → maßgeblichen Stichtag der Teilauszahlung in Ihrem Vertrag auf die Fonds\n",
      "verteilt ist.d) Der → maßgebliche Stichtag für die Berechnung der Teilauszahlung\n",
      "ist der gewünschte Termin oder der zweite → Handelstag, nachdem wir Ihren Antrag\n",
      "auf Teilauszahlung erhalten haben, je nachdem, welcher Termin später ist.Ist der\n",
      "Stichtag kein → Handelstag, ist der nächstmögliche → Handelstag maßgeblich.e)\n",
      "Die Veränderungen können Sie in den Dokumenten nachlesen, die Sie bei einer\n",
      "Vertragsänderung von uns bekommen.\n",
      "Page number: 23\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(chunks_and_embeddings[idx][\"chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {chunks_and_embeddings[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxqIbjrzsRAn"
   },
   "source": [
    "Let's functionize the semantic search pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "SuICPzIqSF5M"
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query,\n",
    "                                   convert_to_tensor=True).to(torch.float16)\n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores,\n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "GXtf6LlLSF9Y"
   },
   "outputs": [],
   "source": [
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 chunks_and_embeddings: list[dict]= chunks_and_embeddings,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires chunks_and_embeddings to be formatted in a specific way.\n",
    "    \"\"\"\n",
    "\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(chunks_and_embeddings[index][\"chunk\"])\n",
    "        # Print the page number too so we can reference the pdf further and check the results.\n",
    "        print(f\"Page number: {chunks_and_embeddings[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PspaAaQ33Z7S"
   },
   "source": [
    "Now test our functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "41dxJoPqSGAZ"
   },
   "outputs": [],
   "source": [
    "query = \"was müssen sie bei teilzahlungen beachten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0FlQvjbR_lJ",
    "outputId": "e5b41e70-4d1b-4661-e917-3c8d32a72b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 120 embeddings: 0.00007 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.5581, 0.4788, 0.4587, 0.4153, 0.4048], device='cuda:0',\n",
       "        dtype=torch.float16),\n",
       " tensor([21, 59, 10, 35, 60], device='cuda:0'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QGYf25RR2OQ",
    "outputId": "18c05d0f-8365-4130-f4c8-768ce71207cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 120 embeddings: 0.00012 seconds.\n",
      "Query: was müssen sie bei teilzahlungen beachten\n",
      "\n",
      "Results:\n",
      "Score: 0.5581\n",
      "10 6.3 Wie und wann leisten wir die Auszahlung? 10 6.4 Wer erhält die\n",
      "Auszahlung?10 7 Zuzahlung; Teilauszahlung und Auszahlungsplan; Verlegung des\n",
      "Ablaufdatums; Kündigung 10 7.1 Was müssen Sie beachten, wenn Sie Zuzahlungen\n",
      "leisten möchten? 10 7.2 Was müssen Sie bei Teilauszahlungen beachten?11 7.3 Was\n",
      "ist der Auszahlungsplan? 11 7.4 Können Sie das Ablaufdatum verschieben und was\n",
      "hat das für Folgen? 12 7.5 Was müssen Sie beachten, wenn Sie Ihren Vertrag\n",
      "kündigen? 12 8 Kosten 13 8.1 Welche Kosten fallen für Ihren Vertrag an?\n",
      "Page number: 11\n",
      "\n",
      "\n",
      "Score: 0.4788\n",
      "Wenn Sie sich dazu nicht äußern, verteilen wir Ihre Zuzahlung entsprechend dem\n",
      "Verhältnis, welches Sie bei Vertragsabschluss für Ihren Einmalbeitrag gewählt\n",
      "haben.g) Für jede Zuzahlung kann Startmanagement gewählt werden (5.9).h) Der →\n",
      "maßgebliche Stichtag für die Berechnung der → Anteilseinheiten, die sich aus\n",
      "Ihrer Zuzahlung ergeben, ist der gewünschte Termin oder der zweite → Handelstag,\n",
      "nachdem wir Ihre Zuzahlung erhalten haben, je nachdem, welcher Termin später\n",
      "ist.Ist der Stichtag kein → Handelstag, ist der nächstmögliche → Handelstag\n",
      "maßgeblich.7.2 Was müssen Sie bei Teilauszahlungen beachten? a) Teilauszahlungen\n",
      "sind unter folgenden Voraussetzungen möglich:  Sie müssen sie in → Textform bei\n",
      "uns beantragen. Eine Teilauszahlung ist frühestens einen Monat nach\n",
      "Vertragsabschluss und spätestens einen Monat vor Vertragsablauf möglich. Es ist\n",
      "nur eine Teilauszahlung monatlich möglich, jedoch nicht mehr als zwei im Jahr.\n",
      "Page number: 23\n",
      "\n",
      "\n",
      "Score: 0.4587\n",
      "Steuerinformationen Seite 2 von 3 /D/1006/XIII/03/22 1.3 Was müssen Sie bei\n",
      "Vertragsänderungen beachten?Bitte beachten Sie auch, dass Änderungen an Ihrem\n",
      "Vertrag dazu führen können, dass Zahlungen aus einer Kapitallebensversicherung\n",
      "im Erlebensfall, bei Teilauszahlung sowie bei Rückkauf des Vertrags ganz oder\n",
      "teilweise mit dem vollen Unterschiedsbetrag der Steuerpflicht unterliegen. 1.4\n",
      "Veräußerung einer Versicherungspolice Bei Veräußerung einer Lebensversicherung\n",
      "müssen wir als Versicherungsunternehmen nach § 20 Abs. 2 Nr. 6 EStG eine Meldung\n",
      "an das Finanzamt, das für den Steuerpflichtigen (= Veräußerer) zuständig ist,\n",
      "vornehmen.Auf Verlangen des Steuerpflichtigen stellen wir eine Bescheinigung\n",
      "über die Höhe der entrichteten Beiträge zum Zeitpunkt der Veräußerung aus.1.5\n",
      "Steuerpflichtiger Steuerpflichtiger im Sinne des § 20 Abs. 1 Nr. 6 EStG ist\n",
      "grundsätzlich derjenige, der das Kapital in Form der Sparanteile im eigenen\n",
      "Namen und für eigene Rechnung dem Versicherungsunternehmen zur Nutzung\n",
      "überlassen hat. In der Regel sind Versicherungsnehmer Steuerpflichtige – bei\n",
      "zwei Versicherungsnehmern entsprechend ihren Anteilen am Vertrag –, da sie die\n",
      "Sparanteile zur Nutzung überlassen haben und auch Inhaber des Rechts sind, die\n",
      "Versicherungsleistung zu fordern. Mit der Einräumung eines unwiderruflichen\n",
      "Bezugsrechts für die steuerpflichtige Versicherungsleistung gilt grundsätzlich\n",
      "der Bezugsberechtigte als Steuerpflichtiger der erzielten Erträge. 1.6\n",
      "Erbschaftsteuer und Schenkungsteuer Versicherungsleistungen, die an\n",
      "Versicherungsnehmer selbst gezahlt werden, sind erbschaftsteuerfrei.\n",
      "Page number: 8\n",
      "\n",
      "\n",
      "Score: 0.4153\n",
      "Wir bleiben aber auch in diesem Fall zur Leistung verpflichtet, wenn uns\n",
      "nachgewiesen wird, dass Sie die Nichtzahlung nicht zu vertreten haben.\n",
      "Page number: 15\n",
      "\n",
      "\n",
      "Score: 0.4048\n",
      " Nach jeder Teilauszahlung müssen noch 5.000 Euro im Vertrag verbleiben. Eine\n",
      "Teilauszahlung muss mindestens 1.500 Euro betragen, maximal jedoch 1.000.000\n",
      "Euro.b) Durch eine Teilauszahlung reduzieren sich die Versicherungsleistungen\n",
      "und die Anteile an den Fonds, die Sie gewählt haben.c) Sie können entscheiden,\n",
      "aus welchen Fonds Sie die Teilauszahlung entnehmen möchten.Tun Sie das nicht,\n",
      "entnehmen wir die Beträge entsprechend dem Verhältnis, in dem Ihr Fondsvermögen\n",
      "am → maßgeblichen Stichtag der Teilauszahlung in Ihrem Vertrag auf die Fonds\n",
      "verteilt ist.d) Der → maßgebliche Stichtag für die Berechnung der Teilauszahlung\n",
      "ist der gewünschte Termin oder der zweite → Handelstag, nachdem wir Ihren Antrag\n",
      "auf Teilauszahlung erhalten haben, je nachdem, welcher Termin später ist.Ist der\n",
      "Stichtag kein → Handelstag, ist der nächstmögliche → Handelstag maßgeblich.e)\n",
      "Die Veränderungen können Sie in den Dokumenten nachlesen, die Sie bei einer\n",
      "Vertragsänderung von uns bekommen.\n",
      "Page number: 23\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vArndjfZFuDu"
   },
   "source": [
    "## Getting an LLM for local generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjQBUv9Z414M"
   },
   "source": [
    "To generate text, we will use a Large Language Model (LLM), which is designed to produce an output when given an input. In our case, we need the LLM to create text based on a text input, ensuring that the output is relevant to the context of the query. The input to an LLM is commonly called a prompt, and we will enhance our prompt by including a query and relevant context from our textbook related to that query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLoLf1DR51o_"
   },
   "source": [
    "Generally, in deep learning models, the higher the number of parameters, the better the model performs (except from overfitting cases). It is tempting to go for the largest size model  but I have 16 GM GPU available, so I have chosen the model \"gemma-2b-it\" which is part of Google's Gemma family of large language models, and used half-precision floating-point format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "J6EVCSyTSksf"
   },
   "outputs": [],
   "source": [
    "attn_implementation = \"sdpa\"\n",
    "use_quantization_config = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EqI-CDZSkui",
    "outputId": "a35a7169-588d-47aa-ce60-af561d40077a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "# Pick a model\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "print(f\"[INFO] Using model_id: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331,
     "referenced_widgets": [
      "c1dcc180a8724c79bc207a59f52199dc",
      "ce85cd7dfdd944058e59b4c96f385c7b",
      "f1619ad96a094fcc9f64afa45e559af3",
      "f2e2397276df4b608d202c9cb1b76148",
      "38a12f2eae1b42568626c9fd984d5a55",
      "64ea6f6e49ee4200b6b1fbe298144a49",
      "0f4c0a27b8b7495b8d58c1e2d5e1f494",
      "c6d2ce389f8f4de3b67bf0a785542721",
      "ab1b293be4604caba73225594bd5827e",
      "ed53da71bb404a6da982ab64ddde22e4",
      "317c9ef08aba439baeda30e4db6f719b",
      "d57b41badbd548c595e2580447ed87d0",
      "35eaa4bcb823408f86212475ad0bf731",
      "35ae2cc6e9ca4b9589131c961bb40260",
      "54642a06976841dcb36c327072123551",
      "5ff8a883eb1f4abdb6d7f3492a18e398",
      "6a535eee2a784077af9c3f29b305dcb8"
     ]
    },
    "id": "hi2Sgxd4SxEe",
    "outputId": "e2ffb8e6-8157-4cd3-d339-9f2e85bc3b3d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dcc180a8724c79bc207a59f52199dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KOVZaNdceSP"
   },
   "source": [
    "can use the token = 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Gs8FuFZ9SxJQ"
   },
   "outputs": [],
   "source": [
    "# Instantiate tokenizer (tokenizer turns text into numbers ready for the model).\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500,
     "referenced_widgets": [
      "d3222ae8e33444f7a2b5e3965cfdf2bc",
      "4555f6e2585644019e6ee1924460bacd",
      "00da89263f054e639fee41af1e7ffcd3",
      "7779ca2e8ff9401ab9488b4bfb4ef3e1",
      "9f4dc47688004bd288ee79716bdcc1a5",
      "e141574a2b714a07b8ed70cfb7b3c0cb",
      "818d641ebba74f66a6b727541fee9834",
      "31ddbebeec7b46f3bc104779e2a55790",
      "1e52a8bab8e54b8ba7eab7fa9dc88286",
      "700e38369c0f4a50babba8ebaa074ed9",
      "e66337c0111f4c5ca16e4b69f474dac6"
     ]
    },
    "id": "Fy-lkgubSxLV",
    "outputId": "379dea2c-7c48-459c-b311-45535f4ed016"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3222ae8e33444f7a2b5e3965cfdf2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 quantization_config= None,\n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "llm_model.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PL4YxAOx9CMW"
   },
   "source": [
    "Le's find out the number of parameters of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4JlifoFxRnX",
    "outputId": "8beb48ef-e578-4224-a327-3952a8b46d04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506172416"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOKH6Ql08A3T"
   },
   "source": [
    "The number of parameters of our model is approximately 2.5 billion. It would be 3 times bigger if we chose to use \"google/gemma-7b-it\" with a higher GPU memory available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sINV_leixYQc",
    "outputId": "3de61431-be1e-4262-8ef8-628e1e59c472"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 5012354048, 'model_mem_mb': 4780.15, 'model_mem_gb': 4.67}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnl8mXTYxwW3",
    "outputId": "a53906b5-7737-4882-a5c2-dd9d8bdc508b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "was müssen sie bei teilzahlungen beachten<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBfpysUJ2Ct6",
    "outputId": "6e0b768c-f8f5-4c92-f93c-7d56dfe8cbcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (tokenized):\n",
      "{'input_ids': tensor([[    2,     2,   106,  1645,   108,  6570, 25374,  4021,  4719, 43659,\n",
      "         26084,  4360, 91095,   107,   108,   106,  2516,   108]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "\n",
      "Model output (tokens):\n",
      "tensor([     2,      2,    106,   1645,    108,   6570,  25374,   4021,   4719,\n",
      "         43659,  26084,   4360,  91095,    107,    108,    106,   2516,    108,\n",
      "           688, 154359,   6842,  24343,    688,    109, 235287,   5231,   1232,\n",
      "         66058,   8654,   5649,    848,  20969,  10945,  11863, 235265,    108,\n",
      "        235287,   5231,  16779,  33305,  66058,   3804,  96802,    848,  20969,\n",
      "         10945,  11863, 235265,    108, 235287,   5231,  50347,  44238,  66058,\n",
      "          5991,  58414,  44238,    848,  20969,  10945,  11863, 235265,    108,\n",
      "        235287,   5231, 235291, 235290,  13617, 235290,  55531,  66058,   3804,\n",
      "           637, 235290,  13617, 235290,  55531,    848,  20969,  10945,  11863,\n",
      "        235265,    109,    688, 195178,   6595,  45759,    688,    109, 235287,\n",
      "          5231,  31445,    477,  38605,  66058,   5991,  36371,    477,  38605,\n",
      "        235269,   1188,   2943,   1303, 109076,  36803,   6183, 235265,    108,\n",
      "        235287,   5231,   3593,    738,   1078,  44238,  66058,   3804,   3776,\n",
      "           738,   1078,  44238, 235269,   1303,   9050, 187419,  40178,   2943,\n",
      "          1303,   3776,  51617,   1743,   9959,   9907, 235265,    108, 235287,\n",
      "          5231,  11122,   2086,  66058,   5991,  84595,   6477,   2086,   1188,\n",
      "         20969, 106226, 235265,    108, 235287,   5231,  49608,   1188, 109076,\n",
      "         66058,   8654,  86793,   1188, 109076, 235265,    108, 235287,   5231,\n",
      "        195178,   6595,    760,  66058,   3804,   5936,   1188, 187419,  77424,\n",
      "           591, 235306, 235265,    599,   1173,  16490, 113062, 235269, 105555,\n",
      "         52537,    846,    109,    688, 195178,   6595,  13613,    688,    109,\n",
      "        235287,   5231,   3593,    738,   1078,  44238,  66058,   3804,   3776,\n",
      "           738,   1078,  44238, 235269,   1303,   9050, 187419,  40178,   2943,\n",
      "          1303,   3776,  51617,   1743,   9959,   9907, 235265,    108, 235287,\n",
      "          5231,  11122,   2086,  66058,   5991,  84595,   6477,   2086,   1188,\n",
      "         20969, 106226, 235265,    108, 235287,   5231,  49608,   1188, 109076,\n",
      "         66058,   8654,  86793,   1188, 109076, 235265,    108, 235287,   5231,\n",
      "        195178,   6595,    760,  66058,   3804,   5936,   1188, 187419,  77424,\n",
      "           591, 235306, 235265,    599,   1173,  16490, 113062, 235269, 105555,\n",
      "         52537,    846,    109,    688, 117035, 201172,  25279,    688,    109,\n",
      "        235287,   5231, 195178,   6595], device='cuda:0')\n",
      "\n",
      "CPU times: user 8.13 s, sys: 0 ns, total: 8.13 s\n",
      "Wall time: 8.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256) # define the maximum number of new tokens to create\n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "DXXpKnC0UQjo"
   },
   "outputs": [],
   "source": [
    "input_text = \"war müssen sie bei teilzahlungen beachten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgGgNtBo2NuC",
    "outputId": "909a42c0-92d1-4d78-d302-1dc0afa90a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<bos><bos><start_of_turn>user\n",
      "was müssen sie bei teilzahlungen beachten<end_of_turn>\n",
      "<start_of_turn>model\n",
      "**Persönliche Daten**\n",
      "\n",
      "* **Name:** Das Name des Teilnehmers.\n",
      "* **Anschrift:** Die Adresse des Teilnehmers.\n",
      "* **Telefonnummer:** Der Telefonnummer des Teilnehmers.\n",
      "* **E-Mail-Adresse:** Die E-Mail-Adresse des Teilnehmers.\n",
      "\n",
      "**Zahlungsdaten**\n",
      "\n",
      "* **Kontonummer:** Der Kontonummer, der für die Zahlung verwendet wird.\n",
      "* **Transaktionsnummer:** Die Transaktionsnummer, die vom Zahlungsdienst für die Transaktion generiert wurde.\n",
      "* **Betrag:** Der Gesamtbetrag der Teilzahlung.\n",
      "* **Datum der Zahlung:** Das Datum der Zahlung.\n",
      "* **Zahlungsart:** Die Art der Zahlungsleistung (z. B., Überweisung, Kreditkarte).\n",
      "\n",
      "**Zahlungsdetails**\n",
      "\n",
      "* **Transaktionsnummer:** Die Transaktionsnummer, die vom Zahlungsdienst für die Transaktion generiert wurde.\n",
      "* **Betrag:** Der Gesamtbetrag der Teilzahlung.\n",
      "* **Datum der Zahlung:** Das Datum der Zahlung.\n",
      "* **Zahlungsart:** Die Art der Zahlungsleistung (z. B., Überweisung, Kreditkarte).\n",
      "\n",
      "**Zusätzliche Informationen**\n",
      "\n",
      "* **Zahlungs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")\n",
    "\n",
    "## the output of this cell is quite general because the augmentation step is not performed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxSNTH-a27eC",
    "outputId": "df9e32e2-f35f-437f-9b4d-114b4195e580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: war müssen sie bei teilzahlungen beachten\n",
      "\n",
      "Output text:\n",
      "**Persönliche Daten**\n",
      "\n",
      "* **Name:** Das Name des Teilnehmers.\n",
      "* **Anschrift:** Die Adresse des Teilnehmers.\n",
      "* **Telefonnummer:** Der Telefonnummer des Teilnehmers.\n",
      "* **E-Mail-Adresse:** Die E-Mail-Adresse des Teilnehmers.\n",
      "\n",
      "**Zahlungsdaten**\n",
      "\n",
      "* **Kontonummer:** Der Kontonummer, der für die Zahlung verwendet wird.\n",
      "* **Transaktionsnummer:** Die Transaktionsnummer, die vom Zahlungsdienst für die Transaktion generiert wurde.\n",
      "* **Betrag:** Der Gesamtbetrag der Teilzahlung.\n",
      "* **Datum der Zahlung:** Das Datum der Zahlung.\n",
      "* **Zahlungsart:** Die Art der Zahlungsleistung (z. B., Überweisung, Kreditkarte).\n",
      "\n",
      "**Zahlungsdetails**\n",
      "\n",
      "* **Transaktionsnummer:** Die Transaktionsnummer, die vom Zahlungsdienst für die Transaktion generiert wurde.\n",
      "* **Betrag:** Der Gesamtbetrag der Teilzahlung.\n",
      "* **Datum der Zahlung:** Das Datum der Zahlung.\n",
      "* **Zahlungsart:** Die Art der Zahlungsleistung (z. B., Überweisung, Kreditkarte).\n",
      "\n",
      "**Zusätzliche Informationen**\n",
      "\n",
      "* **Zahlungs\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input text: {input_text}\\n\")\n",
    "print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")\n",
    "\n",
    "## the output of this cell is quite general because the augmentation step is not performed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlVVRMJw4-Kw",
    "outputId": "6a1fd1dc-581e-4361-ecb7-2dba62b43fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 120 embeddings: 0.00009 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.5581, 0.4788, 0.4587, 0.4153, 0.4048], device='cuda:0',\n",
       "        dtype=torch.float16),\n",
       " tensor([21, 59, 10, 35, 60], device='cuda:0'))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNGFoIJ5Blp2"
   },
   "source": [
    "## Augmenting our prompt\n",
    "We'll utilize the results from our search for relevant resources to enhance the prompt that we pass to our LLM. Essentially, we start with a base prompt and augment it with contextual text.\n",
    "To achieve this, we will write a function called prompt_formatter that takes a query and a list of context items (selected indices from our list of dictionaries inside chunks_and_embeddings) and then formats the query with text from these context items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "VSj0zwfrW2FT"
   },
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str,\n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"chunk\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model. (Because \"google/gemma-2b-it\" has been tranied in an instruction tuned manner, we should follow the instruction template for the best results.)\n",
    "    base_prompt = \"\"\"Ich werde kontextuelle Elemente geben. Diese Elemente enthalten normalerweise Frage-Antwort-Paare. Beantworten Sie die Hauptfrage, die ich Ihnen stelle, unter Verwendung dieser Antworten. Geben Sie keine Gegenfrage als Antwort auf die Frage. Ich möchte nur die Antwort.:\n",
    "    \\nBenutzen Sie nun die folgenden Kontextelemente, um die Benutzeranfrage zu beantworten. Gib nur eine Antwort:\n",
    "    {Kontext}\n",
    "    \\nRelevante Passagen: <Extrahieren Sie hier relevante Passagen aus dem Kontext>\n",
    "    Benutzerabfrage: {Abfrage}\n",
    "    Antwort:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query\n",
    "    base_prompt = base_prompt.format(Kontext=context, Abfrage=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PK-yw9S_MDZ",
    "outputId": "26d90ff1-0652-4bf5-c2c6-84513fb554c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: was müssen sie bei teilzahlungen beachten\n",
      "[INFO] Time taken to get scores on 120 embeddings: 0.00007 seconds.\n",
      "<bos><start_of_turn>user\n",
      "Ich werde kontextuelle Elemente geben. Diese Elemente enthalten normalerweise Frage-Antwort-Paare. Beantworten Sie die Hauptfrage, die ich Ihnen stelle, unter Verwendung dieser Antworten. Geben Sie keine Gegenfrage als Antwort auf die Frage. Ich möchte nur die Antwort.:\n",
      "    \n",
      "Benutzen Sie nun die folgenden Kontextelemente, um die Benutzeranfrage zu beantworten. Gib nur eine Antwort:\n",
      "    - 10 6.3 Wie und wann leisten wir die Auszahlung? 10 6.4 Wer erhält die Auszahlung?10 7 Zuzahlung; Teilauszahlung und Auszahlungsplan; Verlegung des Ablaufdatums; Kündigung 10 7.1 Was müssen Sie beachten, wenn Sie Zuzahlungen leisten möchten? 10 7.2 Was müssen Sie bei Teilauszahlungen beachten?11 7.3 Was ist der Auszahlungsplan? 11 7.4 Können Sie das Ablaufdatum verschieben und was hat das für Folgen? 12 7.5 Was müssen Sie beachten, wenn Sie Ihren Vertrag kündigen? 12 8 Kosten 13 8.1 Welche Kosten fallen für Ihren Vertrag an?\n",
      "- Wenn Sie sich dazu nicht äußern, verteilen wir Ihre Zuzahlung entsprechend dem Verhältnis, welches Sie bei Vertragsabschluss für Ihren Einmalbeitrag gewählt haben.g) Für jede Zuzahlung kann Startmanagement gewählt werden (5.9).h) Der → maßgebliche Stichtag für die Berechnung der → Anteilseinheiten, die sich aus Ihrer Zuzahlung ergeben, ist der gewünschte Termin oder der zweite → Handelstag, nachdem wir Ihre Zuzahlung erhalten haben, je nachdem, welcher Termin später ist.Ist der Stichtag kein → Handelstag, ist der nächstmögliche → Handelstag maßgeblich.7.2 Was müssen Sie bei Teilauszahlungen beachten? a) Teilauszahlungen sind unter folgenden Voraussetzungen möglich:  Sie müssen sie in → Textform bei uns beantragen. Eine Teilauszahlung ist frühestens einen Monat nach Vertragsabschluss und spätestens einen Monat vor Vertragsablauf möglich. Es ist nur eine Teilauszahlung monatlich möglich, jedoch nicht mehr als zwei im Jahr.\n",
      "- Steuerinformationen Seite 2 von 3 /D/1006/XIII/03/22 1.3 Was müssen Sie bei Vertragsänderungen beachten?Bitte beachten Sie auch, dass Änderungen an Ihrem Vertrag dazu führen können, dass Zahlungen aus einer Kapitallebensversicherung im Erlebensfall, bei Teilauszahlung sowie bei Rückkauf des Vertrags ganz oder teilweise mit dem vollen Unterschiedsbetrag der Steuerpflicht unterliegen. 1.4 Veräußerung einer Versicherungspolice Bei Veräußerung einer Lebensversicherung müssen wir als Versicherungsunternehmen nach § 20 Abs. 2 Nr. 6 EStG eine Meldung an das Finanzamt, das für den Steuerpflichtigen (= Veräußerer) zuständig ist, vornehmen.Auf Verlangen des Steuerpflichtigen stellen wir eine Bescheinigung über die Höhe der entrichteten Beiträge zum Zeitpunkt der Veräußerung aus.1.5 Steuerpflichtiger Steuerpflichtiger im Sinne des § 20 Abs. 1 Nr. 6 EStG ist grundsätzlich derjenige, der das Kapital in Form der Sparanteile im eigenen Namen und für eigene Rechnung dem Versicherungsunternehmen zur Nutzung überlassen hat. In der Regel sind Versicherungsnehmer Steuerpflichtige – bei zwei Versicherungsnehmern entsprechend ihren Anteilen am Vertrag –, da sie die Sparanteile zur Nutzung überlassen haben und auch Inhaber des Rechts sind, die Versicherungsleistung zu fordern. Mit der Einräumung eines unwiderruflichen Bezugsrechts für die steuerpflichtige Versicherungsleistung gilt grundsätzlich der Bezugsberechtigte als Steuerpflichtiger der erzielten Erträge. 1.6 Erbschaftsteuer und Schenkungsteuer Versicherungsleistungen, die an Versicherungsnehmer selbst gezahlt werden, sind erbschaftsteuerfrei.\n",
      "- Wir bleiben aber auch in diesem Fall zur Leistung verpflichtet, wenn uns nachgewiesen wird, dass Sie die Nichtzahlung nicht zu vertreten haben.\n",
      "-  Nach jeder Teilauszahlung müssen noch 5.000 Euro im Vertrag verbleiben. Eine Teilauszahlung muss mindestens 1.500 Euro betragen, maximal jedoch 1.000.000 Euro.b) Durch eine Teilauszahlung reduzieren sich die Versicherungsleistungen und die Anteile an den Fonds, die Sie gewählt haben.c) Sie können entscheiden, aus welchen Fonds Sie die Teilauszahlung entnehmen möchten.Tun Sie das nicht, entnehmen wir die Beträge entsprechend dem Verhältnis, in dem Ihr Fondsvermögen am → maßgeblichen Stichtag der Teilauszahlung in Ihrem Vertrag auf die Fonds verteilt ist.d) Der → maßgebliche Stichtag für die Berechnung der Teilauszahlung ist der gewünschte Termin oder der zweite → Handelstag, nachdem wir Ihren Antrag auf Teilauszahlung erhalten haben, je nachdem, welcher Termin später ist.Ist der Stichtag kein → Handelstag, ist der nächstmögliche → Handelstag maßgeblich.e) Die Veränderungen können Sie in den Dokumenten nachlesen, die Sie bei einer Vertragsänderung von uns bekommen.\n",
      "    \n",
      "Relevante Passagen: <Extrahieren Sie hier relevante Passagen aus dem Kontext>\n",
      "    Benutzerabfrage: was müssen sie bei teilzahlungen beachten\n",
      "    Antwort:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get relevant resources\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "\n",
    "# Create a list of context items\n",
    "context_items = [chunks_and_embeddings[i] for i in indices]\n",
    "\n",
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlCFayfV_MGE",
    "outputId": "21460965-aaeb-42f5-866a-6eeff1dfcd03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: was müssen sie bei teilzahlungen beachten\n",
      "RAG answer:\n",
      "<bos>1. Bei teilzahlungen müssen sie in Textform ihre Zuzahlung in die Finanzamt stellen.\n",
      "2. Bei Teilauszahlungen sind sie unter folgenden Voraussetzungen möglich:\n",
      "   a) Sie müssen sie in → Textform bei dem Finanzamt beantragen.\n",
      "   b) Eine Teilauszahlung ist frühestens einen Monat nach Vertragsabschluss und spätestens einen Monat vor Vertragsablauf möglich.\n",
      "   c) Es ist nur eine Teilauszahlung monatlich möglich, jedoch nicht mehr als zwei im Jahr.<eos>\n",
      "CPU times: user 3.64 s, sys: 0 ns, total: 3.64 s\n",
      "Wall time: 3.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True,\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt\n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "gYJeFkmP_MIg"
   },
   "outputs": [],
   "source": [
    "def ask(query,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True,\n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "\n",
    "    # Create a list of context items\n",
    "    context_items = [chunks_and_embeddings[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU\n",
    "\n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query,\n",
    "                              context_items=context_items)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "\n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2A_t5U3LCSG"
   },
   "source": [
    "Adjust the query as you wish and run the cell below to get the final answer generated by our model. The context items and the related information can also be printed if needed by the command \"print(context_items)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azvMlmRd_qC3",
    "outputId": "29035c85-aea4-4b3b-b44e-38b55b26b35c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 120 embeddings: 0.00007 seconds.\n",
      "Query:\n",
      "\n",
      "was müssen sie bei teilzahlungen beachten\n",
      "Answer:\n",
      "\n",
      "1. Bei teilzahlungen müssen sie in Textform Ihre Zuzahlung in die Versicherung\n",
      "beantragen. 2. Bei Teilauszahlungen sind sie unter folgenden Voraussetzungen\n",
      "möglich: a) Sie müssen sie in → Textform bei uns beantragen. b) Eine\n",
      "Teilauszahlung ist frühestens einen Monat nach Vertragsabschluss und spätestens\n",
      "einen Monat vor Vertragsablauf möglich. c) Es ist nur eine Teilauszahlung\n",
      "monatlich möglich, jedoch nicht mehr als zwei im Jahr.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "answer, context_items = ask(query,\n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "\n",
    "print(f\"Query:\\n\")\n",
    "print_wrapped(query)\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "yB4H-uy1_qFf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "2ue3DagK_qH-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00da89263f054e639fee41af1e7ffcd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31ddbebeec7b46f3bc104779e2a55790",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1e52a8bab8e54b8ba7eab7fa9dc88286",
      "value": 2
     }
    },
    "0f4c0a27b8b7495b8d58c1e2d5e1f494": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "1e52a8bab8e54b8ba7eab7fa9dc88286": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "317c9ef08aba439baeda30e4db6f719b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31ddbebeec7b46f3bc104779e2a55790": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35ae2cc6e9ca4b9589131c961bb40260": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35eaa4bcb823408f86212475ad0bf731": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "38a12f2eae1b42568626c9fd984d5a55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_35ae2cc6e9ca4b9589131c961bb40260",
      "style": "IPY_MODEL_54642a06976841dcb36c327072123551",
      "tooltip": ""
     }
    },
    "4555f6e2585644019e6ee1924460bacd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e141574a2b714a07b8ed70cfb7b3c0cb",
      "placeholder": "​",
      "style": "IPY_MODEL_818d641ebba74f66a6b727541fee9834",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "54642a06976841dcb36c327072123551": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "5ff8a883eb1f4abdb6d7f3492a18e398": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64ea6f6e49ee4200b6b1fbe298144a49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ff8a883eb1f4abdb6d7f3492a18e398",
      "placeholder": "​",
      "style": "IPY_MODEL_6a535eee2a784077af9c3f29b305dcb8",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "6a535eee2a784077af9c3f29b305dcb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "700e38369c0f4a50babba8ebaa074ed9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7779ca2e8ff9401ab9488b4bfb4ef3e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_700e38369c0f4a50babba8ebaa074ed9",
      "placeholder": "​",
      "style": "IPY_MODEL_e66337c0111f4c5ca16e4b69f474dac6",
      "value": " 2/2 [00:01&lt;00:00,  1.22it/s]"
     }
    },
    "818d641ebba74f66a6b727541fee9834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f4dc47688004bd288ee79716bdcc1a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab1b293be4604caba73225594bd5827e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1dcc180a8724c79bc207a59f52199dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce85cd7dfdd944058e59b4c96f385c7b",
       "IPY_MODEL_f1619ad96a094fcc9f64afa45e559af3",
       "IPY_MODEL_f2e2397276df4b608d202c9cb1b76148",
       "IPY_MODEL_38a12f2eae1b42568626c9fd984d5a55",
       "IPY_MODEL_64ea6f6e49ee4200b6b1fbe298144a49"
      ],
      "layout": "IPY_MODEL_0f4c0a27b8b7495b8d58c1e2d5e1f494"
     }
    },
    "c6d2ce389f8f4de3b67bf0a785542721": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce85cd7dfdd944058e59b4c96f385c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6d2ce389f8f4de3b67bf0a785542721",
      "placeholder": "​",
      "style": "IPY_MODEL_ab1b293be4604caba73225594bd5827e",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "d3222ae8e33444f7a2b5e3965cfdf2bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4555f6e2585644019e6ee1924460bacd",
       "IPY_MODEL_00da89263f054e639fee41af1e7ffcd3",
       "IPY_MODEL_7779ca2e8ff9401ab9488b4bfb4ef3e1"
      ],
      "layout": "IPY_MODEL_9f4dc47688004bd288ee79716bdcc1a5"
     }
    },
    "d57b41badbd548c595e2580447ed87d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e141574a2b714a07b8ed70cfb7b3c0cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e66337c0111f4c5ca16e4b69f474dac6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed53da71bb404a6da982ab64ddde22e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1619ad96a094fcc9f64afa45e559af3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_ed53da71bb404a6da982ab64ddde22e4",
      "placeholder": "​",
      "style": "IPY_MODEL_317c9ef08aba439baeda30e4db6f719b",
      "value": ""
     }
    },
    "f2e2397276df4b608d202c9cb1b76148": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_d57b41badbd548c595e2580447ed87d0",
      "style": "IPY_MODEL_35eaa4bcb823408f86212475ad0bf731",
      "value": true
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
